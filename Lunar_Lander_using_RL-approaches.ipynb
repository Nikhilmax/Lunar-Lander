{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_state(state):\n",
    "    dstate = (min(2, max(-2, int((state[0]) / 0.05))), \\\n",
    "                        min(2, max(-2, int((state[1]) / 0.1))), \\\n",
    "                        min(2, max(-2, int((state[2]) / 0.1))), \\\n",
    "                        min(2, max(-2, int((state[3]) / 0.1))), \\\n",
    "                        min(2, max(-2, int((state[4]) / 0.1))), \\\n",
    "                        min(2, max(-2, int((state[5]) / 0.1))), \\\n",
    "                        int(state[6]), \\\n",
    "                        int(state[7]))\n",
    "\n",
    "    return dstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsgreedy(qfunc, state,epsilon,actions):\n",
    "    prob = np.random.random()\n",
    "    if prob<epsilon:\n",
    "        return random.choice(range(actions))\n",
    "    else:\n",
    "        qvals = [qfunc[state+(action, )]for action in range(actions)] \n",
    "        return np.argmax(qvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(qstatesdict,state,actions):\n",
    "    qvals = [qstatesdict[state+(action, )]for action in range(actions)]\n",
    "    return max(qvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted(eps_return,gamma):\n",
    "    g=0\n",
    "    for i, r in enumerate(eps_return):\n",
    "        g+=gamma**i * r\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decayedeps(curreps,exploration_f_epsilon):\n",
    "    if curreps<exploration_f_epsilon:\n",
    "        return curreps\n",
    "    else:\n",
    "        return curreps*0.996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-09 11:41:33,566] Making new env: LunarLander-v2\n",
      "/home/nihkil/.local/lib/python3.8/site-packages/gym/envs/registration.py:18: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lander(env, numofepisodes,gamma,lr,min_epsilon,printfreq = 500,renderfreq = 500):\n",
    "    qstates = collections.defaultdict(float)\n",
    "    rewardperepisode = [0.0]\n",
    "    epsilon = 1.0\n",
    "    numofactions = env.action_space.n\n",
    "    for i in range(numofepisodes):\n",
    "        t= 0\n",
    "        if(i+1)%renderfreq==0:\n",
    "            render =True\n",
    "        else:\n",
    "            render = False\n",
    "        currentstate = disc_state(env.reset())\n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "            #choosing Action and State using behaviour policy\n",
    "            action = epsgreedy(qstates,currentstate,epsilon,numofactions)\n",
    "            #creating state action pair  \n",
    "            qstate = currentstate+(action, )\n",
    "\n",
    "            #first take the action and get reward then goto next state\n",
    "            #state-perform action-get reward-next state\n",
    "            observations, reward, done,_= env.step(action)\n",
    "            nextstate = disc_state(observations)\n",
    "\n",
    "            if not done:\n",
    "                qstates[qstate] += lr*(reward+gamma*greedy(qstates,nextstate,numofactions)-qstates[qstate]) \n",
    "            else:\n",
    "                qstates[qstate] += lr*(reward-qstates[qstate])\n",
    "                \n",
    "\n",
    "    \n",
    "            rewardperepisode[-1] += reward\n",
    "            if done:\n",
    "                if(i+1)%printfreq==0:\n",
    "                    print(\"\\nEpisode Finisehed after {} timesteps\".format(t+1))\n",
    "                    print(\"Episode {}: Total Return = {}\".format(i+1,rewardperepisode[-1]))\n",
    "                    print(\"Total keys in q_states dictionary = {}\".format(len(qstates)))\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    mean_100ep_reward = round(np.mean(rewardperepisode[-101:-1]), 1)\n",
    "                    print(\"Last 100 episodes mean reward: {}\".format(mean_100ep_reward))\n",
    "                epsilon = decayedeps(epsilon,min_epsilon)\n",
    "                rewardperepisode.append(0.0)\n",
    "\n",
    "                break\n",
    "            currentstate=nextstate\n",
    "            t+=1\n",
    "    return rewardperepisode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10000\n",
    "lr = 0.1\n",
    "gamma = 0.99\n",
    "final_eps = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 episodes mean reward: -192.4\n",
      "Last 100 episodes mean reward: -167.1\n",
      "Last 100 episodes mean reward: -142.5\n",
      "Last 100 episodes mean reward: -160.9\n",
      "\n",
      "Episode Finisehed after 120 timesteps\n",
      "Episode 500: Total Return = -98.34676344548346\n",
      "Total keys in q_states dictionary = 10496\n",
      "Last 100 episodes mean reward: -178.8\n",
      "Last 100 episodes mean reward: -174.3\n",
      "Last 100 episodes mean reward: -178.5\n",
      "Last 100 episodes mean reward: -214.7\n",
      "Last 100 episodes mean reward: -134.4\n",
      "\n",
      "Episode Finisehed after 238 timesteps\n",
      "Episode 1000: Total Return = -81.8898105123693\n",
      "Total keys in q_states dictionary = 15144\n",
      "Last 100 episodes mean reward: -176.7\n",
      "Last 100 episodes mean reward: -164.5\n",
      "Last 100 episodes mean reward: -162.5\n",
      "Last 100 episodes mean reward: -105.9\n",
      "Last 100 episodes mean reward: -102.5\n",
      "\n",
      "Episode Finisehed after 606 timesteps\n",
      "Episode 1500: Total Return = 146.99402155594396\n",
      "Total keys in q_states dictionary = 21528\n",
      "Last 100 episodes mean reward: -84.5\n",
      "Last 100 episodes mean reward: -107.4\n",
      "Last 100 episodes mean reward: -85.1\n",
      "Last 100 episodes mean reward: -45.7\n",
      "Last 100 episodes mean reward: -64.3\n",
      "\n",
      "Episode Finisehed after 240 timesteps\n",
      "Episode 2000: Total Return = -72.70178772653378\n",
      "Total keys in q_states dictionary = 26176\n",
      "Last 100 episodes mean reward: -87.2\n",
      "Last 100 episodes mean reward: -61.1\n",
      "Last 100 episodes mean reward: -43.9\n",
      "Last 100 episodes mean reward: -15.1\n",
      "Last 100 episodes mean reward: 10.1\n",
      "\n",
      "Episode Finisehed after 322 timesteps\n",
      "Episode 2500: Total Return = -164.08784357114985\n",
      "Total keys in q_states dictionary = 29900\n",
      "Last 100 episodes mean reward: -5.4\n",
      "Last 100 episodes mean reward: -28.5\n",
      "Last 100 episodes mean reward: -0.3\n",
      "Last 100 episodes mean reward: -13.0\n",
      "Last 100 episodes mean reward: -17.1\n",
      "\n",
      "Episode Finisehed after 423 timesteps\n",
      "Episode 3000: Total Return = 187.74461894631963\n",
      "Total keys in q_states dictionary = 31768\n",
      "Last 100 episodes mean reward: 28.0\n",
      "Last 100 episodes mean reward: 32.8\n",
      "Last 100 episodes mean reward: 30.7\n",
      "Last 100 episodes mean reward: 49.7\n",
      "Last 100 episodes mean reward: 80.7\n",
      "\n",
      "Episode Finisehed after 620 timesteps\n",
      "Episode 3500: Total Return = 156.50827940602548\n",
      "Total keys in q_states dictionary = 33592\n",
      "Last 100 episodes mean reward: 72.9\n",
      "Last 100 episodes mean reward: 62.9\n",
      "Last 100 episodes mean reward: 60.6\n",
      "Last 100 episodes mean reward: 84.1\n",
      "Last 100 episodes mean reward: 87.7\n",
      "\n",
      "Episode Finisehed after 660 timesteps\n",
      "Episode 4000: Total Return = 149.99021430164242\n",
      "Total keys in q_states dictionary = 34528\n",
      "Last 100 episodes mean reward: 91.3\n",
      "Last 100 episodes mean reward: 96.9\n",
      "Last 100 episodes mean reward: 83.2\n",
      "Last 100 episodes mean reward: 94.1\n",
      "Last 100 episodes mean reward: 85.9\n",
      "\n",
      "Episode Finisehed after 823 timesteps\n",
      "Episode 4500: Total Return = 119.87832161080294\n",
      "Total keys in q_states dictionary = 35332\n",
      "Last 100 episodes mean reward: 79.8\n",
      "Last 100 episodes mean reward: 86.5\n",
      "Last 100 episodes mean reward: 95.5\n",
      "Last 100 episodes mean reward: 107.7\n",
      "Last 100 episodes mean reward: 104.7\n",
      "\n",
      "Episode Finisehed after 657 timesteps\n",
      "Episode 5000: Total Return = -80.97415713636006\n",
      "Total keys in q_states dictionary = 35672\n",
      "Last 100 episodes mean reward: 117.7\n",
      "Last 100 episodes mean reward: 112.6\n",
      "Last 100 episodes mean reward: 100.6\n",
      "Last 100 episodes mean reward: 103.7\n",
      "Last 100 episodes mean reward: 101.6\n",
      "\n",
      "Episode Finisehed after 588 timesteps\n",
      "Episode 5500: Total Return = 159.18458546512133\n",
      "Total keys in q_states dictionary = 36000\n",
      "Last 100 episodes mean reward: 113.1\n",
      "Last 100 episodes mean reward: 113.0\n",
      "Last 100 episodes mean reward: 111.9\n",
      "Last 100 episodes mean reward: 123.0\n",
      "Last 100 episodes mean reward: 113.3\n",
      "\n",
      "Episode Finisehed after 781 timesteps\n",
      "Episode 6000: Total Return = 112.08193807297884\n",
      "Total keys in q_states dictionary = 36192\n",
      "Last 100 episodes mean reward: 103.6\n",
      "Last 100 episodes mean reward: 107.4\n",
      "Last 100 episodes mean reward: 116.9\n",
      "Last 100 episodes mean reward: 102.2\n",
      "Last 100 episodes mean reward: 107.0\n",
      "\n",
      "Episode Finisehed after 830 timesteps\n",
      "Episode 6500: Total Return = 127.23822447334895\n",
      "Total keys in q_states dictionary = 36436\n",
      "Last 100 episodes mean reward: 72.9\n",
      "Last 100 episodes mean reward: 95.4\n",
      "Last 100 episodes mean reward: 93.8\n",
      "Last 100 episodes mean reward: 102.3\n",
      "Last 100 episodes mean reward: 117.4\n",
      "\n",
      "Episode Finisehed after 625 timesteps\n",
      "Episode 7000: Total Return = 183.6433479545703\n",
      "Total keys in q_states dictionary = 36792\n",
      "Last 100 episodes mean reward: 128.7\n",
      "Last 100 episodes mean reward: 123.3\n",
      "Last 100 episodes mean reward: 129.6\n",
      "Last 100 episodes mean reward: 135.5\n",
      "Last 100 episodes mean reward: 119.9\n",
      "\n",
      "Episode Finisehed after 746 timesteps\n",
      "Episode 7500: Total Return = 72.64949020775401\n",
      "Total keys in q_states dictionary = 36924\n",
      "Last 100 episodes mean reward: 131.6\n",
      "Last 100 episodes mean reward: 126.6\n",
      "Last 100 episodes mean reward: 122.2\n",
      "Last 100 episodes mean reward: 122.1\n",
      "Last 100 episodes mean reward: 119.5\n",
      "\n",
      "Episode Finisehed after 669 timesteps\n",
      "Episode 8000: Total Return = 139.1582635522921\n",
      "Total keys in q_states dictionary = 37052\n",
      "Last 100 episodes mean reward: 126.0\n",
      "Last 100 episodes mean reward: 124.5\n",
      "Last 100 episodes mean reward: 105.5\n",
      "Last 100 episodes mean reward: 120.3\n",
      "Last 100 episodes mean reward: 118.1\n",
      "\n",
      "Episode Finisehed after 670 timesteps\n",
      "Episode 8500: Total Return = 105.73998794710558\n",
      "Total keys in q_states dictionary = 37248\n",
      "Last 100 episodes mean reward: 123.1\n",
      "Last 100 episodes mean reward: 121.6\n",
      "Last 100 episodes mean reward: 119.0\n",
      "Last 100 episodes mean reward: 130.6\n",
      "Last 100 episodes mean reward: 117.3\n",
      "\n",
      "Episode Finisehed after 840 timesteps\n",
      "Episode 9000: Total Return = 128.00357534842382\n",
      "Total keys in q_states dictionary = 37460\n",
      "Last 100 episodes mean reward: 130.4\n",
      "Last 100 episodes mean reward: 131.9\n",
      "Last 100 episodes mean reward: 119.8\n",
      "Last 100 episodes mean reward: 124.3\n",
      "Last 100 episodes mean reward: 120.9\n",
      "\n",
      "Episode Finisehed after 750 timesteps\n",
      "Episode 9500: Total Return = 123.20385418712537\n",
      "Total keys in q_states dictionary = 37608\n",
      "Last 100 episodes mean reward: 127.4\n",
      "Last 100 episodes mean reward: 112.4\n",
      "Last 100 episodes mean reward: 113.7\n",
      "Last 100 episodes mean reward: 122.7\n",
      "Last 100 episodes mean reward: 134.5\n",
      "\n",
      "Episode Finisehed after 715 timesteps\n",
      "Episode 10000: Total Return = 117.60439578265755\n",
      "Total keys in q_states dictionary = 37764\n",
      "Last 100 episodes mean reward: 130.4\n"
     ]
    }
   ],
   "source": [
    "totalrewards = lander(env,n_episodes,gamma,lr,final_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def montecarlo_lander(env,numofepisodes,gamma,lr,min_epsilon,printfreq = 500,renderfreq = 500):\n",
    "    qstates = collections.defaultdict(float)\n",
    "    n_visits = collections.defaultdict(int)\n",
    "    rewardperepisode = [0.0]\n",
    "    epsilon = 1.0\n",
    "    numofactions = env.action_space.n\n",
    "    epis_qstates = []\n",
    "    epis_return_reward = []\n",
    "    for i in range(numofepisodes):\n",
    "        t = 0\n",
    "        curr_state = disc_state(env.reset())\n",
    "        if (i + 1) % renderfreq == 0:\n",
    "            render = True\n",
    "        else:\n",
    "            render = False\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # choose action A using ε-greedy policy\n",
    "            action = epsgreedy(qstates, curr_state, epsilon, numofactions)\n",
    "    \n",
    "            # take action A, earn immediate reward and land into next state S'\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "    \n",
    "            qstate = curr_state + (action, )\n",
    "            epis_qstates.append(qstate)\n",
    "    \n",
    "            # increment visit count = N(state, action)\n",
    "            n_visits[qstate] += 1\n",
    "    \n",
    "            rewardperepisode[-1] += reward\n",
    "            epis_return_reward.append(reward)\n",
    "    \n",
    "            if done:\n",
    "                if (i + 1) % printfreq == 0:\n",
    "                    print(\"\\nEpisode finished after {} timesteps\".format(t+1))\n",
    "                    print(\"Episode {}: Total return = {}\".format(i + 1, rewardperepisode[-1]))\n",
    "                    print(\"Total keys in q_states dictionary = {}\".format(len(qstates)))\n",
    "                    print(\"Total keys in n_visits dictionary = {}\".format(len(n_visits)))\n",
    "\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    mean_100ep_reward = round(np.mean(rewardperepisode[-101:-1]), 1)\n",
    "                    print(\"Last 100 episodes mean reward: {}\".format(mean_100ep_reward))\n",
    "    \n",
    "                #Policy evaluation step\n",
    "                # improve policy only when episode has terminated\n",
    "                for step, qstate in enumerate(epis_qstates):\n",
    "                    qstates[qstate] += (discounted(epis_return_reward[step: ], gamma) - qstates[qstate]) / n_visits[qstate]\n",
    "                \n",
    "\n",
    "                epsilon = decayedeps(epsilon, min_epsilon)\n",
    "                rewardperepisode.append(0.0)\n",
    "                epis_qstates.clear()\n",
    "                epis_return_reward.clear()\n",
    "    \n",
    "                break\n",
    "    \n",
    "            curr_state = disc_state(observation)\n",
    "            t += 1\n",
    "\n",
    "    return rewardperepisode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10000\n",
    "lr = 0.01\n",
    "gamma = 0.99\n",
    "final_eps = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 episodes mean reward: -194.7\n",
      "Last 100 episodes mean reward: -169.8\n",
      "Last 100 episodes mean reward: -146.7\n",
      "Last 100 episodes mean reward: -154.9\n",
      "\n",
      "Episode finished after 336 timesteps\n",
      "Episode 500: Total return = -183.98156855281042\n",
      "Total keys in q_states dictionary = 5670\n",
      "Total keys in n_visits dictionary = 4060\n",
      "Last 100 episodes mean reward: -139.8\n",
      "Last 100 episodes mean reward: -148.9\n",
      "Last 100 episodes mean reward: -141.2\n",
      "Last 100 episodes mean reward: -145.4\n",
      "Last 100 episodes mean reward: -138.8\n",
      "\n",
      "Episode finished after 531 timesteps\n",
      "Episode 1000: Total return = 113.15752387979842\n",
      "Total keys in q_states dictionary = 7864\n",
      "Total keys in n_visits dictionary = 5317\n",
      "Last 100 episodes mean reward: -142.4\n",
      "Last 100 episodes mean reward: -145.3\n",
      "Last 100 episodes mean reward: -139.1\n",
      "Last 100 episodes mean reward: -137.4\n",
      "Last 100 episodes mean reward: -142.7\n",
      "\n",
      "Episode finished after 150 timesteps\n",
      "Episode 1500: Total return = -152.82107728039722\n",
      "Total keys in q_states dictionary = 9586\n",
      "Total keys in n_visits dictionary = 6206\n",
      "Last 100 episodes mean reward: -148.4\n",
      "Last 100 episodes mean reward: -140.1\n",
      "Last 100 episodes mean reward: -138.4\n",
      "Last 100 episodes mean reward: -140.5\n",
      "Last 100 episodes mean reward: -130.9\n",
      "\n",
      "Episode finished after 93 timesteps\n",
      "Episode 2000: Total return = -113.95425720863321\n",
      "Total keys in q_states dictionary = 10750\n",
      "Total keys in n_visits dictionary = 6850\n",
      "Last 100 episodes mean reward: -139.6\n",
      "Last 100 episodes mean reward: -135.5\n",
      "Last 100 episodes mean reward: -138.5\n",
      "Last 100 episodes mean reward: -141.8\n",
      "Last 100 episodes mean reward: -135.1\n",
      "\n",
      "Episode finished after 281 timesteps\n",
      "Episode 2500: Total return = -190.91946495965928\n",
      "Total keys in q_states dictionary = 11785\n",
      "Total keys in n_visits dictionary = 7384\n",
      "Last 100 episodes mean reward: -132.9\n",
      "Last 100 episodes mean reward: -136.5\n",
      "Last 100 episodes mean reward: -130.2\n",
      "Last 100 episodes mean reward: -121.6\n",
      "Last 100 episodes mean reward: -121.1\n",
      "\n",
      "Episode finished after 87 timesteps\n",
      "Episode 3000: Total return = -127.62938909189853\n",
      "Total keys in q_states dictionary = 14025\n",
      "Total keys in n_visits dictionary = 8548\n",
      "Last 100 episodes mean reward: -127.5\n",
      "Last 100 episodes mean reward: -124.5\n",
      "Last 100 episodes mean reward: -120.4\n",
      "Last 100 episodes mean reward: -134.7\n",
      "Last 100 episodes mean reward: -127.4\n",
      "\n",
      "Episode finished after 113 timesteps\n",
      "Episode 3500: Total return = -98.03084265733241\n",
      "Total keys in q_states dictionary = 15724\n",
      "Total keys in n_visits dictionary = 9508\n",
      "Last 100 episodes mean reward: -136.4\n",
      "Last 100 episodes mean reward: -125.8\n",
      "Last 100 episodes mean reward: -142.8\n",
      "Last 100 episodes mean reward: -125.0\n",
      "Last 100 episodes mean reward: -122.2\n",
      "\n",
      "Episode finished after 295 timesteps\n",
      "Episode 4000: Total return = -165.50022975472442\n",
      "Total keys in q_states dictionary = 17159\n",
      "Total keys in n_visits dictionary = 10383\n",
      "Last 100 episodes mean reward: -139.7\n",
      "Last 100 episodes mean reward: -121.1\n",
      "Last 100 episodes mean reward: -107.7\n",
      "Last 100 episodes mean reward: -114.3\n",
      "Last 100 episodes mean reward: -127.1\n",
      "\n",
      "Episode finished after 176 timesteps\n",
      "Episode 4500: Total return = -56.858454883519\n",
      "Total keys in q_states dictionary = 18724\n",
      "Total keys in n_visits dictionary = 11236\n",
      "Last 100 episodes mean reward: -104.3\n",
      "Last 100 episodes mean reward: -123.4\n",
      "Last 100 episodes mean reward: -121.3\n",
      "Last 100 episodes mean reward: -126.8\n",
      "Last 100 episodes mean reward: -133.1\n",
      "\n",
      "Episode finished after 196 timesteps\n",
      "Episode 5000: Total return = -91.11544898651199\n",
      "Total keys in q_states dictionary = 19733\n",
      "Total keys in n_visits dictionary = 11870\n",
      "Last 100 episodes mean reward: -122.7\n",
      "Last 100 episodes mean reward: -118.1\n",
      "Last 100 episodes mean reward: -105.8\n",
      "Last 100 episodes mean reward: -120.1\n",
      "Last 100 episodes mean reward: -123.3\n",
      "\n",
      "Episode finished after 377 timesteps\n",
      "Episode 5500: Total return = -147.38802233013666\n",
      "Total keys in q_states dictionary = 20899\n",
      "Total keys in n_visits dictionary = 12526\n",
      "Last 100 episodes mean reward: -124.8\n",
      "Last 100 episodes mean reward: -105.0\n",
      "Last 100 episodes mean reward: -105.4\n",
      "Last 100 episodes mean reward: -121.9\n",
      "Last 100 episodes mean reward: -118.7\n",
      "\n",
      "Episode finished after 608 timesteps\n",
      "Episode 6000: Total return = 134.1610761376938\n",
      "Total keys in q_states dictionary = 22131\n",
      "Total keys in n_visits dictionary = 13211\n",
      "Last 100 episodes mean reward: -101.0\n",
      "Last 100 episodes mean reward: -119.3\n",
      "Last 100 episodes mean reward: -119.9\n",
      "Last 100 episodes mean reward: -118.7\n",
      "Last 100 episodes mean reward: -119.3\n",
      "\n",
      "Episode finished after 100 timesteps\n",
      "Episode 6500: Total return = -56.367350826450526\n",
      "Total keys in q_states dictionary = 23076\n",
      "Total keys in n_visits dictionary = 13821\n",
      "Last 100 episodes mean reward: -113.5\n",
      "Last 100 episodes mean reward: -116.1\n",
      "Last 100 episodes mean reward: -120.0\n",
      "Last 100 episodes mean reward: -102.6\n",
      "Last 100 episodes mean reward: -106.6\n",
      "\n",
      "Episode finished after 138 timesteps\n",
      "Episode 7000: Total return = -72.26996046998147\n",
      "Total keys in q_states dictionary = 23949\n",
      "Total keys in n_visits dictionary = 14378\n",
      "Last 100 episodes mean reward: -134.8\n",
      "Last 100 episodes mean reward: -114.4\n",
      "Last 100 episodes mean reward: -108.2\n",
      "Last 100 episodes mean reward: -112.4\n",
      "Last 100 episodes mean reward: -91.9\n",
      "\n",
      "Episode finished after 161 timesteps\n",
      "Episode 7500: Total return = -85.91889999241758\n",
      "Total keys in q_states dictionary = 24995\n",
      "Total keys in n_visits dictionary = 14965\n",
      "Last 100 episodes mean reward: -110.6\n",
      "Last 100 episodes mean reward: -106.5\n",
      "Last 100 episodes mean reward: -99.4\n",
      "Last 100 episodes mean reward: -103.5\n",
      "Last 100 episodes mean reward: -110.1\n",
      "\n",
      "Episode finished after 89 timesteps\n",
      "Episode 8000: Total return = -130.37181627538132\n",
      "Total keys in q_states dictionary = 25632\n",
      "Total keys in n_visits dictionary = 15380\n",
      "Last 100 episodes mean reward: -117.9\n",
      "Last 100 episodes mean reward: -111.0\n",
      "Last 100 episodes mean reward: -122.3\n",
      "Last 100 episodes mean reward: -110.4\n",
      "Last 100 episodes mean reward: -122.8\n",
      "\n",
      "Episode finished after 116 timesteps\n",
      "Episode 8500: Total return = -144.9631816187478\n",
      "Total keys in q_states dictionary = 26132\n",
      "Total keys in n_visits dictionary = 15722\n",
      "Last 100 episodes mean reward: -106.2\n",
      "Last 100 episodes mean reward: -122.2\n",
      "Last 100 episodes mean reward: -114.2\n",
      "Last 100 episodes mean reward: -128.3\n",
      "Last 100 episodes mean reward: -105.4\n",
      "\n",
      "Episode finished after 131 timesteps\n",
      "Episode 9000: Total return = -165.35624458935482\n",
      "Total keys in q_states dictionary = 26772\n",
      "Total keys in n_visits dictionary = 16112\n",
      "Last 100 episodes mean reward: -108.2\n",
      "Last 100 episodes mean reward: -107.5\n",
      "Last 100 episodes mean reward: -116.9\n",
      "Last 100 episodes mean reward: -120.1\n",
      "Last 100 episodes mean reward: -115.1\n",
      "\n",
      "Episode finished after 145 timesteps\n",
      "Episode 9500: Total return = -154.11657647814522\n",
      "Total keys in q_states dictionary = 27369\n",
      "Total keys in n_visits dictionary = 16477\n",
      "Last 100 episodes mean reward: -111.0\n",
      "Last 100 episodes mean reward: -112.8\n",
      "Last 100 episodes mean reward: -107.4\n",
      "Last 100 episodes mean reward: -103.9\n",
      "Last 100 episodes mean reward: -93.4\n",
      "\n",
      "Episode finished after 82 timesteps\n",
      "Episode 10000: Total return = -144.07406903167993\n",
      "Total keys in q_states dictionary = 27955\n",
      "Total keys in n_visits dictionary = 16873\n",
      "Last 100 episodes mean reward: -109.8\n"
     ]
    }
   ],
   "source": [
    "mc_rewards = montecarlo_lander(env,n_episodes,gamma,lr,final_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_lander(env,numofepisodes,gamma,lr,min_epsilon,printfreq = 500,renderfreq = 500):\n",
    "    qstates = collections.defaultdict(float)\n",
    "    rewardperepisode = [0.0]\n",
    "    numofactions = env.action_space.n\n",
    "    epsilon = 1.0\n",
    "\n",
    "    for i in range(numofepisodes):\n",
    "        t = 0\n",
    "        if (i+1)%renderfreq==0:\n",
    "            render = True\n",
    "        else:\n",
    "            render = False\n",
    "        \n",
    "        currentstate = disc_state(env.reset())\n",
    "        action = epsgreedy(qstates,currentstate,epsilon,numofactions)\n",
    "\n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            #Creating State Action pair\n",
    "            qstate = currentstate+(action, )\n",
    "\n",
    "            #perform Action and recieve Reward and goto next state\n",
    "            # Current state--perform Action--get reward--goto next state\n",
    "\n",
    "            observation,reward,done,_ = env.step(action)\n",
    "            nextstate = disc_state(observation)\n",
    "\n",
    "            #getting Next state\n",
    "            #Choosing next Action to goto next State\n",
    "\n",
    "            nextaction = epsgreedy(qstates,nextstate,epsilon,numofactions)\n",
    "\n",
    "            #Policy Evaluation Step\n",
    "            if not done:\n",
    "                qstates[qstate]+=lr * (reward + gamma * qstates[qstate] - qstates[qstate]) #Its not final state\n",
    "            else:\n",
    "                qstates[qstate]+=lr * (reward - qstates[qstate]) #Reached Goal\n",
    "            \n",
    "            rewardperepisode[-1] += reward\n",
    "\n",
    "            if done:\n",
    "                if(i+1)%printfreq==0:\n",
    "                    print(\"\\nEpisode finished after {} timesteps\".format(t + 1))\n",
    "                    print(\"Episode {}: Total Return = {}\".format(i + 1, rewardperepisode[-1]))\n",
    "                    print(\"Total keys in q_states dictionary = {}\".format(len(qstates)))\n",
    "\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    mean_100ep_reward = round(np.mean(rewardperepisode[-101:-1]), 1)\n",
    "                    print(\"Last 100 episodes mean reward: {}\".format(mean_100ep_reward))\n",
    "\n",
    "                epsilon = decayedeps(epsilon, min_epsilon)\n",
    "                rewardperepisode.append(0.0)\n",
    "\n",
    "                break\n",
    "\n",
    "            currentstate = nextstate\n",
    "            action = nextaction\n",
    "            t += 1\n",
    "\n",
    "    return rewardperepisode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10000\n",
    "lr = 0.01\n",
    "gamma = 0.99\n",
    "final_eps = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 episodes mean reward: -201.1\n",
      "Last 100 episodes mean reward: -182.5\n",
      "Last 100 episodes mean reward: -196.9\n",
      "Last 100 episodes mean reward: -227.0\n",
      "\n",
      "Episode finished after 206 timesteps\n",
      "Episode 500: Total Return = -282.01281949924913\n",
      "Total keys in q_states dictionary = 7428\n",
      "Last 100 episodes mean reward: -232.6\n",
      "Last 100 episodes mean reward: -239.4\n",
      "Last 100 episodes mean reward: -263.6\n",
      "Last 100 episodes mean reward: -267.8\n",
      "Last 100 episodes mean reward: -264.6\n",
      "\n",
      "Episode finished after 142 timesteps\n",
      "Episode 1000: Total Return = -281.92518482393007\n",
      "Total keys in q_states dictionary = 9254\n",
      "Last 100 episodes mean reward: -267.9\n",
      "Last 100 episodes mean reward: -249.2\n",
      "Last 100 episodes mean reward: -268.4\n",
      "Last 100 episodes mean reward: -252.2\n",
      "Last 100 episodes mean reward: -256.5\n",
      "\n",
      "Episode finished after 159 timesteps\n",
      "Episode 1500: Total Return = -248.84351289362573\n",
      "Total keys in q_states dictionary = 10410\n",
      "Last 100 episodes mean reward: -256.8\n",
      "Last 100 episodes mean reward: -261.4\n",
      "Last 100 episodes mean reward: -252.7\n",
      "Last 100 episodes mean reward: -249.3\n",
      "Last 100 episodes mean reward: -260.3\n",
      "\n",
      "Episode finished after 196 timesteps\n",
      "Episode 2000: Total Return = -332.6912158634273\n",
      "Total keys in q_states dictionary = 11054\n",
      "Last 100 episodes mean reward: -257.0\n",
      "Last 100 episodes mean reward: -244.2\n",
      "Last 100 episodes mean reward: -245.1\n",
      "Last 100 episodes mean reward: -252.3\n",
      "Last 100 episodes mean reward: -273.8\n",
      "\n",
      "Episode finished after 117 timesteps\n",
      "Episode 2500: Total Return = -431.22981450656573\n",
      "Total keys in q_states dictionary = 11699\n",
      "Last 100 episodes mean reward: -236.3\n",
      "Last 100 episodes mean reward: -254.5\n",
      "Last 100 episodes mean reward: -247.6\n",
      "Last 100 episodes mean reward: -249.4\n",
      "Last 100 episodes mean reward: -242.3\n",
      "\n",
      "Episode finished after 142 timesteps\n",
      "Episode 3000: Total Return = -284.62751786980664\n",
      "Total keys in q_states dictionary = 12283\n",
      "Last 100 episodes mean reward: -268.7\n",
      "Last 100 episodes mean reward: -233.9\n",
      "Last 100 episodes mean reward: -255.0\n",
      "Last 100 episodes mean reward: -236.8\n",
      "Last 100 episodes mean reward: -247.2\n",
      "\n",
      "Episode finished after 182 timesteps\n",
      "Episode 3500: Total Return = -180.3439968791366\n",
      "Total keys in q_states dictionary = 12722\n",
      "Last 100 episodes mean reward: -259.8\n",
      "Last 100 episodes mean reward: -239.8\n",
      "Last 100 episodes mean reward: -257.3\n",
      "Last 100 episodes mean reward: -260.5\n",
      "Last 100 episodes mean reward: -240.0\n",
      "\n",
      "Episode finished after 120 timesteps\n",
      "Episode 4000: Total Return = -370.948252194249\n",
      "Total keys in q_states dictionary = 13127\n",
      "Last 100 episodes mean reward: -256.2\n",
      "Last 100 episodes mean reward: -250.1\n",
      "Last 100 episodes mean reward: -250.3\n",
      "Last 100 episodes mean reward: -238.1\n",
      "Last 100 episodes mean reward: -228.1\n",
      "\n",
      "Episode finished after 131 timesteps\n",
      "Episode 4500: Total Return = -292.44244851538394\n",
      "Total keys in q_states dictionary = 13555\n",
      "Last 100 episodes mean reward: -249.3\n",
      "Last 100 episodes mean reward: -237.3\n",
      "Last 100 episodes mean reward: -231.3\n",
      "Last 100 episodes mean reward: -244.0\n",
      "Last 100 episodes mean reward: -238.6\n",
      "\n",
      "Episode finished after 124 timesteps\n",
      "Episode 5000: Total Return = -169.44329840271962\n",
      "Total keys in q_states dictionary = 13787\n",
      "Last 100 episodes mean reward: -220.7\n",
      "Last 100 episodes mean reward: -246.8\n",
      "Last 100 episodes mean reward: -247.6\n",
      "Last 100 episodes mean reward: -245.6\n",
      "Last 100 episodes mean reward: -245.4\n",
      "\n",
      "Episode finished after 107 timesteps\n",
      "Episode 5500: Total Return = -227.01375099466026\n",
      "Total keys in q_states dictionary = 14107\n",
      "Last 100 episodes mean reward: -239.2\n",
      "Last 100 episodes mean reward: -254.6\n",
      "Last 100 episodes mean reward: -222.2\n",
      "Last 100 episodes mean reward: -240.9\n",
      "Last 100 episodes mean reward: -242.3\n",
      "\n",
      "Episode finished after 174 timesteps\n",
      "Episode 6000: Total Return = -182.6896974011648\n",
      "Total keys in q_states dictionary = 14329\n",
      "Last 100 episodes mean reward: -229.1\n",
      "Last 100 episodes mean reward: -227.0\n",
      "Last 100 episodes mean reward: -236.4\n",
      "Last 100 episodes mean reward: -232.2\n",
      "Last 100 episodes mean reward: -235.5\n",
      "\n",
      "Episode finished after 130 timesteps\n",
      "Episode 6500: Total Return = -280.045430993216\n",
      "Total keys in q_states dictionary = 14531\n",
      "Last 100 episodes mean reward: -257.6\n",
      "Last 100 episodes mean reward: -257.3\n",
      "Last 100 episodes mean reward: -252.2\n",
      "Last 100 episodes mean reward: -224.7\n",
      "Last 100 episodes mean reward: -234.0\n",
      "\n",
      "Episode finished after 158 timesteps\n",
      "Episode 7000: Total Return = -113.79232129485143\n",
      "Total keys in q_states dictionary = 14767\n",
      "Last 100 episodes mean reward: -239.9\n",
      "Last 100 episodes mean reward: -235.7\n",
      "Last 100 episodes mean reward: -218.9\n",
      "Last 100 episodes mean reward: -222.5\n",
      "Last 100 episodes mean reward: -235.3\n",
      "\n",
      "Episode finished after 178 timesteps\n",
      "Episode 7500: Total Return = -260.59554159476016\n",
      "Total keys in q_states dictionary = 15081\n",
      "Last 100 episodes mean reward: -222.9\n",
      "Last 100 episodes mean reward: -227.6\n",
      "Last 100 episodes mean reward: -223.0\n",
      "Last 100 episodes mean reward: -241.3\n",
      "Last 100 episodes mean reward: -219.2\n",
      "\n",
      "Episode finished after 234 timesteps\n",
      "Episode 8000: Total Return = -160.0306120028117\n",
      "Total keys in q_states dictionary = 15404\n",
      "Last 100 episodes mean reward: -209.1\n",
      "Last 100 episodes mean reward: -226.7\n",
      "Last 100 episodes mean reward: -244.8\n",
      "Last 100 episodes mean reward: -230.9\n",
      "Last 100 episodes mean reward: -232.8\n",
      "\n",
      "Episode finished after 122 timesteps\n",
      "Episode 8500: Total Return = -414.45740504415033\n",
      "Total keys in q_states dictionary = 15759\n",
      "Last 100 episodes mean reward: -228.7\n",
      "Last 100 episodes mean reward: -237.0\n",
      "Last 100 episodes mean reward: -224.1\n",
      "Last 100 episodes mean reward: -222.2\n",
      "Last 100 episodes mean reward: -243.1\n",
      "\n",
      "Episode finished after 234 timesteps\n",
      "Episode 9000: Total Return = -262.33369339606924\n",
      "Total keys in q_states dictionary = 16028\n",
      "Last 100 episodes mean reward: -228.5\n",
      "Last 100 episodes mean reward: -227.7\n",
      "Last 100 episodes mean reward: -226.0\n",
      "Last 100 episodes mean reward: -232.6\n",
      "Last 100 episodes mean reward: -212.5\n",
      "\n",
      "Episode finished after 136 timesteps\n",
      "Episode 9500: Total Return = -180.0325718293862\n",
      "Total keys in q_states dictionary = 16272\n",
      "Last 100 episodes mean reward: -225.9\n",
      "Last 100 episodes mean reward: -226.9\n",
      "Last 100 episodes mean reward: -222.2\n",
      "Last 100 episodes mean reward: -237.6\n",
      "Last 100 episodes mean reward: -233.7\n",
      "\n",
      "Episode finished after 228 timesteps\n",
      "Episode 10000: Total Return = -329.3078920416393\n",
      "Total keys in q_states dictionary = 16490\n",
      "Last 100 episodes mean reward: -234.8\n"
     ]
    }
   ],
   "source": [
    "sarsa_rewards = sarsa_lander(env,n_episodes,gamma,lr,final_eps)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
